{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshilj0310/Data-Science-Notebooks/blob/main/Optimization_problems.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYLdoP3BnjSA"
      },
      "source": [
        "# Problem 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zN_-5EgknjSF"
      },
      "source": [
        "## A"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_g_s_rlnjSG"
      },
      "source": [
        "\n",
        "\n",
        "### Problem Overview\n",
        "\n",
        "The task is to determine whether a given dataset \\( D = \\{(x_i, y_i)\\}_{i=1}^{n} \\), where \\( x_i \\in \\mathbb{R}^d \\) and \\( y_i \\in \\{+1, -1\\} \\), can be separated by a linear boundary.\n",
        "\n",
        "A dataset is considered **linearly separable** if there exists a non-zero vector \\( w^* \\in \\mathbb{R}^d \\), a bias term \\( b \\in \\mathbb{R} \\), and a margin \\( \\gamma > 0 \\) such that for every \\( i \\in \\{1, 2, \\dots, n\\} \\), the following inequality holds:\n",
        "\n",
        "\\[\n",
        "y_i (\\langle w^*, x_i \\rangle - b) \\geq \\gamma\n",
        "\\]\n",
        "\n",
        "### Essential Elements of the Problem\n",
        "\n",
        "#### Objective\n",
        "\n",
        "The aim is to check whether there exists a linear separator, represented by \\( w^* \\) and \\( b \\), that divides the data points with a margin of at least \\( \\gamma \\).\n",
        "\n",
        "#### Formulation as an Optimization Problem\n",
        "\n",
        "The problem can be framed as an optimization problem, where we seek to:\n",
        "\n",
        "\\[\n",
        "\\min_{z} f(z) \\quad \\text{subject to} \\quad A z \\leq q\n",
        "\\]\n",
        "\n",
        "Here, \\( z \\) represents the decision variables, while \\( A \\) and \\( q \\) capture the constraints that ensure linear separability.\n",
        "\n",
        "### Steps for Setting Up the Optimization Problem\n",
        "\n",
        "#### Decision Variables \\( z \\)\n",
        "\n",
        "The decision variables consist of the weight vector \\( w \\), the bias \\( b \\), and the margin \\( \\gamma \\). Thus, we define:\n",
        "\n",
        "\\[\n",
        "z = (w, b, \\gamma) \\in \\mathbb{R}^{d+2}\n",
        "\\]\n",
        "\n",
        "where \\( w \\in \\mathbb{R}^d \\), \\( b \\in \\mathbb{R} \\), and \\( \\gamma > 0 \\).\n",
        "\n",
        "#### Constraints\n",
        "\n",
        "The primary condition for linear separability is:\n",
        "\n",
        "\\[\n",
        "y_i (\\langle w, x_i \\rangle - b) \\geq \\gamma\n",
        "\\]\n",
        "\n",
        "This can be rewritten as:\n",
        "\n",
        "\\[\n",
        "y_i (\\langle w, x_i \\rangle - b) - \\gamma \\geq 0\n",
        "\\]\n",
        "\n",
        "For each data point \\( i \\in \\{1, 2, \\dots, n\\} \\), we have the constraint:\n",
        "\n",
        "\\[\n",
        "y_i (\\langle w, x_i \\rangle - b) - \\gamma \\geq 0\n",
        "\\]\n",
        "\n",
        "In matrix form, this can be expressed as:\n",
        "\n",
        "\\[\n",
        "A z \\leq q\n",
        "\\]\n",
        "\n",
        "where \\( A \\in \\mathbb{R}^{n \\times (d+2)} \\) and \\( q \\in \\mathbb{R}^n \\).\n",
        "\n",
        "#### Matrix \\( A \\)\n",
        "\n",
        "The matrix \\( A \\) is structured as follows:\n",
        "\n",
        "\\[\n",
        "A =\n",
        "\\begin{pmatrix}\n",
        " -y_1 x_1^\\top & y_1 & -1 \\\\\n",
        " -y_2 x_2^\\top & y_2 & -1 \\\\\n",
        " \\vdots & \\vdots & \\vdots \\\\\n",
        " -y_n x_n^\\top & y_n & -1\n",
        "\\end{pmatrix}\n",
        "\\]\n",
        "\n",
        "#### Vector \\( q \\)\n",
        "\n",
        "The vector \\( q \\) is a zero vector:\n",
        "\n",
        "\\[\n",
        "q =\n",
        "\\begin{pmatrix}\n",
        "0 \\\\\n",
        "0 \\\\\n",
        "\\vdots \\\\\n",
        "0\n",
        "\\end{pmatrix}\n",
        "\\]\n",
        "\n",
        "These constraints ensure that the dataset is linearly separable.\n",
        "\n",
        "#### Objective Function \\( f(z) \\)\n",
        "\n",
        "Since the main goal is to verify the existence of a linear separator, a natural choice for the objective function is a regularization term that avoids trivial solutions, such as the norm of \\( w \\):\n",
        "\n",
        "\\[\n",
        "f(z) = \\|w\\|^2\n",
        "\\]\n",
        "\n",
        "This function discourages trivial solutions like \\( w = 0 \\) and can also be replaced by any constant since we're only interested in feasibility, not optimization.\n",
        "\n",
        "### The Final Optimization Problem\n",
        "\n",
        "The final optimization problem is structured as follows:\n",
        "\n",
        "\\[\n",
        "\\min_{z} \\|w\\|^2 \\quad \\text{subject to} \\quad A z \\leq q\n",
        "\\]\n",
        "\n",
        "where:\n",
        "- \\( z = (w, b, \\gamma) \\in \\mathbb{R}^{d+2} \\),\n",
        "- \\( A \\in \\mathbb{R}^{n \\times (d+2)} \\) is the constraint matrix, and\n",
        "- \\( q \\in \\mathbb{R}^n \\) is a zero vector.\n",
        "\n",
        "### Matrix and Variable Structure\n",
        "\n",
        "- **Decision Variables \\( z \\)**: \\( z \\in \\mathbb{R}^{d+2} \\), where \\( w \\in \\mathbb{R}^d \\), \\( b \\in \\mathbb{R} \\), and \\( \\gamma \\in \\mathbb{R}^1 \\).\n",
        "- **Matrix \\( A \\)**: \\( A \\in \\mathbb{R}^{n \\times (d+2)} \\), with each row in the form \\( (-y_i x_i^\\top, y_i, -1) \\).\n",
        "- **Vector \\( q \\)**: \\( q \\in \\mathbb{R}^n \\), with all entries set to zero.\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "This optimization framework provides a method to determine whether the dataset is linearly separable. If a feasible solution \\( z = (w, b, \\gamma) \\) exists such that \\( \\gamma > 0 \\), then the dataset is linearly separable. If no such solution can be found, the dataset is not separable. The constraints ensure that the condition \\( y_i (\\langle w, x_i \\rangle - b) \\geq \\gamma \\) is satisfied for all data points.\n",
        "\n",
        "---\n",
        "\n",
        "This version has been rephrased for clarity while maintaining the same meaning. Let me know if you'd like further modifications!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRZQhYWonjSJ"
      },
      "source": [
        "# B"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIcYp5IKnjSK"
      },
      "source": [
        "<p>To implement and solve the optimization problem, we will use <strong>CVXPY</strong>, a popular open-source Python library for convex optimization. This will allow us to check if the dataset is linearly separable using the optimization formulation we derived earlier.</p>\n",
        "\n",
        "<h3>Steps to Implement:</h3>\n",
        "<ol>\n",
        "  <li>Install the <strong>cvxpy</strong> package</li>\n",
        "  <li>Define the decision variables <code>z = (w, b, γ)</code>.</li>\n",
        "  <li>Define the matrix <code>A</code> and vector <code>q</code> as per the optimization formulation.</li>\n",
        "  <li>Implement the objective function <code>f(z) = ∥w∥²</code>.</li>\n",
        "  <li>Solve the optimization problem using CVXPY to check if the dataset is linearly separable.</li>\n",
        "</ol>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwkY6AlDnjSK",
        "outputId": "6ada7fc4-f2b2-40d3-e4cb-fd2860af3a4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: cvxpy in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: osqp>=0.6.2 in /usr/local/lib/python3.10/dist-packages (from cvxpy) (0.6.7.post0)\n",
            "Requirement already satisfied: ecos>=2 in /usr/local/lib/python3.10/dist-packages (from cvxpy) (2.0.14)\n",
            "Requirement already satisfied: clarabel>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from cvxpy) (0.9.0)\n",
            "Requirement already satisfied: scs>=3.2.4.post1 in /usr/local/lib/python3.10/dist-packages (from cvxpy) (3.2.7)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.10/dist-packages (from cvxpy) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from cvxpy) (1.13.1)\n",
            "Requirement already satisfied: qdldl in /usr/local/lib/python3.10/dist-packages (from osqp>=0.6.2->cvxpy) (0.1.7.post4)\n"
          ]
        }
      ],
      "source": [
        "!pip install cvxpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HcCbsAIgnjSO",
        "outputId": "4e1f0393-fc49-440f-afc5-b8e57c0789cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The dataset is linearly separable!\n",
            "Optimal weight vector (w): [3.84732296e-12 2.17605319e-11]\n",
            "Optimal bias (b): -2.053907262666263e-11\n",
            "Optimal margin (gamma): -2.6365280554751554e-11\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import cvxpy as cp\n",
        "\n",
        "\n",
        "n = 5\n",
        "d = 2\n",
        "\n",
        "X = np.array([[1, 2], [2, 3], [3, 3], [4, 5], [1, 1]])\n",
        "y = np.array([1, 1, -1, -1, 1])\n",
        "\n",
        "w = cp.Variable(d)\n",
        "b = cp.Variable()\n",
        "gamma = cp.Variable()\n",
        "\n",
        "\n",
        "constraints = [y[i] * (X[i, :] @ w - b) >= gamma for i in range(n)]\n",
        "constraints.append(gamma >= 0)\n",
        "\n",
        "objective = cp.Minimize(cp.norm(w, 2))\n",
        "\n",
        "problem = cp.Problem(objective, constraints)\n",
        "\n",
        "\n",
        "result = problem.solve()\n",
        "\n",
        "\n",
        "if problem.status == cp.OPTIMAL:\n",
        "    print(f\"The dataset is linearly separable!\")\n",
        "    print(f\"Optimal weight vector (w): {w.value}\")\n",
        "    print(f\"Optimal bias (b): {b.value}\")\n",
        "    print(f\"Optimal margin (gamma): {gamma.value}\")\n",
        "else:\n",
        "    print(f\"The dataset is not linearly separable.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XlOybginjSP"
      },
      "source": [
        "<h3>Explanation:</h3>\n",
        "\n",
        "<h4>Decision Variables:</h4>\n",
        "<p>\n",
        "We define <code>w</code> (weight vector) as a <code>cvxpy.Variable</code> of size <code>d</code>, representing the dimension of each data point.<br>\n",
        "We define <code>b</code> (bias) and <code>γ</code> (margin) as scalar decision variables.\n",
        "</p>\n",
        "\n",
        "<h4>Constraints:</h4>\n",
        "<p>\n",
        "We construct the constraint <code>y<sub>i</sub>(⟨w, x<sub>i</sub>⟩ − b) ≥ γ</code> for each data point <code>i</code>.<br>\n",
        "We also add the constraint <code>γ > 0</code> to ensure the margin is positive.\n",
        "</p>\n",
        "\n",
        "<h4>Objective Function:</h4>\n",
        "<p>\n",
        "We minimize <code>∥w∥<sup>2</sup></code>, the Euclidean norm of the weight vector, as a regularization to ensure we are not trivializing the solution.\n",
        "</p>\n",
        "\n",
        "<h4>Solver:</h4>\n",
        "<p>\n",
        "The optimization problem is solved using the <code>cvxpy.Problem()</code> class and its <code>solve()</code> method.<br>\n",
        "We check the <code>problem.status</code> to see if the problem is <code>OPTIMAL</code>, which would indicate that a linear separator exists, meaning the dataset is linearly separable.\n",
        "</p>\n",
        "\n",
        "<h4>Observations:</h4>\n",
        "<ul>\n",
        "  <li>If the solver returns an optimal solution, the dataset is linearly separable, and the values of <code>w</code>, <code>b</code>, and <code>γ</code> will represent the separating hyperplane and margin.</li>\n",
        "  <li>If the solver cannot find an optimal solution (e.g., <code>problem.status != cp.OPTIMAL</code>), it means the dataset is not linearly separable.</li>\n",
        "</ul>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjR7b0gnnjSP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5V-tQsBSnjSQ"
      },
      "source": [
        "# C"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvR5P0xUHt4z",
        "outputId": "f1ca1142-d0a5-40c6-f096-ad6091682b5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset 1: Linearly separable\n",
            "Dataset 2: Linearly separable\n",
            "Dataset 3: Not linearly separable\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cvxpy as cp\n",
        "\n",
        "# Function to check linear separability using CVXPY\n",
        "def check_linear_separability(data):\n",
        "    # Separate features (first four columns) and labels (last column)\n",
        "    X = data.iloc[:, :-1].values\n",
        "    y = data.iloc[:, -1].values\n",
        "\n",
        "    # Modify labels to be +1 and -1 if they are not already\n",
        "    y = np.where(y == 1, 1, -1)\n",
        "\n",
        "    # Number of samples and features\n",
        "    n, d = X.shape\n",
        "\n",
        "    # Decision variables\n",
        "    w = cp.Variable(d)\n",
        "    b = cp.Variable()\n",
        "    gamma = cp.Variable()\n",
        "\n",
        "    # Define a small epsilon to avoid strict inequality\n",
        "    epsilon = 1e-5\n",
        "\n",
        "    # Define constraints: y_i * (w @ x_i - b) >= gamma for each sample i\n",
        "    constraints = [y[i] * (X[i, :] @ w - b) >= gamma for i in range(n)]\n",
        "    constraints.append(gamma >= epsilon)\n",
        "\n",
        "    # Objective: minimize ||w||^2 (for feasibility checking)\n",
        "    objective = cp.Minimize(cp.norm(w, 2))\n",
        "\n",
        "    # Formulate and solve the problem\n",
        "    problem = cp.Problem(objective, constraints)\n",
        "    result = problem.solve()\n",
        "\n",
        "    # Check if the problem is solved optimally\n",
        "    if problem.status == cp.OPTIMAL:\n",
        "        return \"Linearly separable\"\n",
        "    else:\n",
        "        return \"Not linearly separable\"\n",
        "\n",
        "# Load the datasets\n",
        "data1 = pd.read_csv('data1.csv', names=['a1', 'b1', 'c1', 'd1', 'label'])\n",
        "data2 = pd.read_csv('data2.csv', names=['a2', 'b2', 'c2', 'd2', 'label'])\n",
        "data3 = pd.read_csv('data3.csv', names=['a3', 'b3', 'c3', 'd3', 'label'])\n",
        "\n",
        "# Check linear separability for all three datasets\n",
        "result1 = check_linear_separability(data1)\n",
        "result2 = check_linear_separability(data2)\n",
        "result3 = check_linear_separability(data3)\n",
        "\n",
        "# Output the results\n",
        "print(f\"Dataset 1: {result1}\")\n",
        "print(f\"Dataset 2: {result2}\")\n",
        "print(f\"Dataset 3: {result3}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrZIAbfQJSJ1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "657EgXC_suxU"
      },
      "source": [
        "# D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7vAqUvxJSJ1"
      },
      "source": [
        "### Checking linear seprability by Perceptron algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVMCoxvhJSJ2",
        "outputId": "75e5564c-7178-4408-daa8-9f72876f1d95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data1.csv is linearly separable.\n",
            "data2.csv is linearly separable.\n",
            "data3.csv is not linearly separable.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "class Perceptron:\n",
        "    def __init__(self, learning_rate=0.01, max_iter=1000):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_iter = max_iter\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "\n",
        "        for _ in range(self.max_iter):\n",
        "            for idx, x_i in enumerate(X):\n",
        "                linear_output = np.dot(x_i, self.weights) + self.bias\n",
        "                y_predicted = np.sign(linear_output)\n",
        "\n",
        "                if y_predicted != y[idx]:\n",
        "                    self.weights += self.learning_rate * (y[idx] - y_predicted) * x_i\n",
        "                    self.bias += self.learning_rate * (y[idx] - y_predicted)\n",
        "\n",
        "    def predict(self, X):\n",
        "        linear_output = np.dot(X, self.weights) + self.bias\n",
        "        return np.sign(linear_output)\n",
        "\n",
        "    @staticmethod\n",
        "    def load_data(file_path):\n",
        "        data = pd.read_csv(file_path, header=None)\n",
        "        X = data.iloc[:, :-1].values  # All columns except last as features\n",
        "        y = data.iloc[:, -1].values    # Last column as target\n",
        "        return X, y\n",
        "\n",
        "# List of dataset file paths\n",
        "datasets = ['data1.csv', 'data2.csv', 'data3.csv']\n",
        "\n",
        "# Iterate through each dataset\n",
        "for dataset in datasets:\n",
        "    X, y = Perceptron.load_data(dataset)\n",
        "    y = np.where(y == 1, 1, -1)  # Convert target variable to -1 and 1\n",
        "\n",
        "    # Initialize and fit the perceptron\n",
        "    percep = Perceptron()\n",
        "    percep.fit(X, y)\n",
        "\n",
        "    # Make predictions\n",
        "    predictions = percep.predict(X)\n",
        "\n",
        "    # Check if linearly separable\n",
        "    if np.array_equal(predictions, y):\n",
        "        print(f\"{dataset} is linearly separable.\")\n",
        "    else:\n",
        "        print(f\"{dataset} is not linearly separable.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8zgwQ6lJSJ2"
      },
      "source": [
        "# E"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcW5ortCts3W"
      },
      "source": [
        "\n",
        "\n",
        "### Explanation:\n",
        "\n",
        "#### Perceptron Class:\n",
        "The Perceptron class is responsible for initializing the weights and bias. During each iteration, the algorithm identifies whether any data points are misclassified, meaning they do not satisfy the condition \\( y_i (w \\cdot x_i + b) > 0 \\). When a misclassification occurs, the weights and bias are updated based on the Perceptron update rule. If the algorithm finishes without finding any misclassified points, it indicates that the dataset is linearly separable.\n",
        "\n",
        "#### Verifying Linear Separability:\n",
        "We begin by loading the dataset and splitting it into feature vectors (X) and labels (y). The Perceptron algorithm is then applied to the dataset. If it successfully converges—finding a solution where no points are misclassified—then the dataset is linearly separable. However, if the algorithm reaches the maximum number of iterations without convergence, this suggests that the dataset is not linearly separable.\n",
        "\n",
        "#### Rationale for the Approach:\n",
        "The Perceptron algorithm is guaranteed to converge within a finite number of iterations if the dataset is linearly separable. Failure to converge implies that a linear separator does not exist. In this implementation, the algorithm's ability to converge within the specified maximum iterations serves as an indicator of linear separability. This method is both sound and appropriate, as the Perceptron is a well-established tool for assessing linear separability in datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qOSMsQRbJSJ3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}